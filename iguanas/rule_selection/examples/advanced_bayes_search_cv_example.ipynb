{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d32be5",
   "metadata": {},
   "source": [
    "# Advanced Bayes Search CV Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b6b53",
   "metadata": {},
   "source": [
    "This is a more advanced example of how the `BayesSearchCV` class can be applied - it's recommended that you first read through the simpler `bayes_search_cv_example`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be487a30",
   "metadata": {},
   "source": [
    "The `BayesSearchCV` class is used to search for the set of hyperparameters that produce the best decision engine performance for a given Iguanas Pipeline, whilst also reducing the likelihood of overfitting.\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "* Generate k-fold stratified cross validation datasets. \n",
    "* For each of the training and validation datasets:\n",
    "    * Fit the pipeline on the training set using a set of parameters chosen by the Bayesian Optimiser from a given set of ranges.\n",
    "    * Apply the pipeline to the validation set to return a prediction.\n",
    "    * Use the provided `scorer` to calculate the score of the prediction.\n",
    "* Return the parameter set which generated the highest mean overall score across the validation datasets.\n",
    "\n",
    "In this example, we'll consider the following more advanced workflow (compared to the standard `bayes_search_cv_example` notebook), which considers the generation of a Rules-Based System for a credit card fraud transaction use case:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accdca31",
   "metadata": {},
   "source": [
    "![title](images/complex_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f2f83",
   "metadata": {},
   "source": [
    "Here, we have a fraud detection use case, and we're aiming to create two distinct rule sets - one for flagging fraudulent behaviour; one for flagging good behaviour. Each of these rule sets will be comprised of a generated rule set and an existing rule set. We'll optimise and filter these two rule sets separately, then combine and feed them into the decision engine optimiser. **Note:** we optimise the generated rules as they'll be created using the `RuleGeneratorDT` class, which generates rules from the branches of decision trees - these split based on gini or entropy - so we can further optimise them for a specific metric. \n",
    "\n",
    "**The decision engine will have the following constraint:** for a given transaction, if any approve rules fire it will be approved; else, if any reject rules fire it will be rejected; else, it will be approved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b9f4d",
   "metadata": {},
   "source": [
    "We'll use the `BayesSearchCV` class to optimise the hyperparameters of the steps in this workflow, **ensuring that we maximise the revenue for our decision engine.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aba9dc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438fcc3",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250846dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iguanas.rule_generation import RuleGeneratorDT\n",
    "from iguanas.rule_selection import SimpleFilter, CorrelatedFilter, GreedyFilter, BayesSearchCV\n",
    "from iguanas.metrics import FScore, Precision, Revenue, JaccardSimilarity\n",
    "from iguanas.rbs import RBSOptimiser, RBSPipeline\n",
    "from iguanas.correlation_reduction import AgglomerativeClusteringReducer\n",
    "from iguanas.pipeline import LinearPipeline, ParallelPipeline\n",
    "from iguanas.pipeline.class_accessor import ClassAccessor\n",
    "from iguanas.space import UniformFloat, UniformInteger, Choice\n",
    "from iguanas.rules import Rules\n",
    "from iguanas.rule_optimisation import BayesianOptimiser\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce42ec",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0492642",
   "metadata": {},
   "source": [
    "Let's read in the [credit card fraud dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud) from Kaggle.\n",
    "\n",
    "**Note:** this data has been altered to include some null values in the `V1` column. This is to simulate unprocessed data (the dataset on Kaggle has been processed using PCA, so there are no null values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3cba0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Class'\n",
    "time_col = 'Time'\n",
    "amt_col = 'Amount'\n",
    "# Ready in data\n",
    "df = pd.read_pickle('dummy_data/creditcard.pkl')\n",
    "# Sort data by time ascending\n",
    "df.sort_values(time_col, ascending=True)\n",
    "# Create X and y dataframes\n",
    "X = df.drop([target_col, time_col], axis=1)\n",
    "y = df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7bdbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "183c324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "amts_train = X_train_raw[amt_col]\n",
    "amts_test = X_test_raw[amt_col]\n",
    "X_train_raw = X_train_raw.drop([amt_col], axis=1)\n",
    "X_test_raw = X_test_raw.drop([amt_col], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cc9323",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d3646",
   "metadata": {},
   "source": [
    "Let's impute the null values with the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3354798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train_raw),\n",
    "    columns=X_train_raw.columns,\n",
    "    index=X_train_raw.index\n",
    ")\n",
    "X_test = pd.DataFrame(\n",
    "    imputer.transform(X_test_raw),\n",
    "    columns=X_test_raw.columns,\n",
    "    index=X_test_raw.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d8a9f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check nulls have been imputed\n",
    "X_train.isna().sum().sum(), X_test.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871bcd75",
   "metadata": {},
   "source": [
    "### Existing rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa86f82a",
   "metadata": {},
   "source": [
    "Let's also assume we have the following existing rules, stored in the standard Iguanas string format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "960e33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_rule_strings = {\n",
    "    \"ExistingReject1\": \"((X['V1']<0)|(X['V1'].isna()))&(X['V3']<1)\",\n",
    "    \"ExistingReject2\": \"(X['V2']>3)\",\n",
    "}\n",
    "good_rule_strings = {\n",
    "    \"ExistingApprove1\": \"(X['V1']>0)&(X['V3']>1)\",\n",
    "    \"ExistingApprove2\": \"(X['V2']<3)\",\n",
    "    \"ExistingApprove3\": \"(X['V4']<3)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d16bb",
   "metadata": {},
   "source": [
    "We can create a `Rules` class for each of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5cdef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_rules = Rules(rule_strings=fraud_rule_strings)\n",
    "good_rules = Rules(rule_strings=good_rule_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1324448",
   "metadata": {},
   "source": [
    "Then convert them to the standard Iguanas lambda expression format (we'll need this for the optimisation step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4bb205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_rule_lambdas = fraud_rules.as_rule_lambdas(\n",
    "    as_numpy=False, \n",
    "    with_kwargs=True\n",
    ")\n",
    "good_rule_lambdas = good_rules.as_rule_lambdas(\n",
    "    as_numpy=False, \n",
    "    with_kwargs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e38dd",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc7364",
   "metadata": {},
   "source": [
    "## Set up pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67c9cc",
   "metadata": {},
   "source": [
    "Before we can apply the `BayesSearchCV` class, we need to set up our pipeline. To create the workflow shown at the beginning of the notebook, we must use a combination of `LinearPipeline` and `ParallelPipeline` classes as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8f534",
   "metadata": {},
   "source": [
    "![title](images/complex_example_setup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c65b20b",
   "metadata": {},
   "source": [
    "Let's begin building the **Fraud *LinearPipeline***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c02beb",
   "metadata": {},
   "source": [
    "### Fraud *LinearPipeline*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc1ca6",
   "metadata": {},
   "source": [
    "Let's first instantiate the classes that we'll use in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11021686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "f1 = FScore(beta=1)\n",
    "# Precision\n",
    "p = Precision()\n",
    "    \n",
    "# Rule generation\n",
    "fraud_gen = RuleGeneratorDT(\n",
    "    metric=f1.fit,\n",
    "    n_total_conditions=4,\n",
    "    tree_ensemble=RandomForestClassifier(\n",
    "        n_estimators=10,\n",
    "        random_state=0\n",
    "    ),\n",
    "    target_feat_corr_types='Infer',\n",
    "    rule_name_prefix='Reject' # Set this so generated reject rules distinguishable from approve rules\n",
    ")\n",
    "\n",
    "# Rule optimisation (for generated rules)\n",
    "fraud_gen_opt = BayesianOptimiser(\n",
    "    rule_lambdas=ClassAccessor(\n",
    "        class_tag='fraud_gen',\n",
    "        class_attribute='rule_lambdas'\n",
    "    ),\n",
    "    lambda_kwargs=ClassAccessor(\n",
    "        class_tag='fraud_gen',\n",
    "        class_attribute='lambda_kwargs'\n",
    "    ),\n",
    "    metric=f1.fit,\n",
    "    n_iter=10\n",
    ")\n",
    "\n",
    "# Rule optimisation (for existing rules)\n",
    "fraud_opt = BayesianOptimiser(\n",
    "    rule_lambdas=fraud_rule_lambdas,\n",
    "    lambda_kwargs=fraud_rules.lambda_kwargs,\n",
    "    metric=f1.fit,\n",
    "    n_iter=10\n",
    ")\n",
    "\n",
    "# Rule filter (performance-based)\n",
    "fraud_sf = SimpleFilter(\n",
    "    threshold=0.1, \n",
    "    operator='>=', \n",
    "    metric=f1.fit\n",
    ")\n",
    "\n",
    "# Rule filter (correlation-based)\n",
    "js = JaccardSimilarity()\n",
    "fraud_cf = CorrelatedFilter(\n",
    "    correlation_reduction_class=AgglomerativeClusteringReducer(\n",
    "        threshold=0.9, \n",
    "        strategy='top_down', \n",
    "        similarity_function=js.fit, \n",
    "        metric=f1.fit\n",
    "    ), \n",
    "    rules=ClassAccessor(\n",
    "        class_tag='fraud_gen',\n",
    "        class_attribute='rules'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Rule filter (greedy-based)\n",
    "fraud_gf = GreedyFilter(\n",
    "    metric=f1.fit,\n",
    "    sorting_metric=p.fit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5446d91",
   "metadata": {},
   "source": [
    "Now we can create our **Fraud Rule Generation *LinearPipeline***. Note that we pass the tag for the optimisation of the generated rules to the `use_init_data` parameter, so that the feature set is passed to the `BayesianOptimiser` class, rather than the output from the `RuleGeneratorDT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bb0c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_gen_lp = LinearPipeline(\n",
    "    steps = [\n",
    "        ('fraud_gen', fraud_gen),\n",
    "        ('fraud_gen_opt', fraud_gen_opt),\n",
    "    ],\n",
    "    use_init_data=['fraud_gen_opt']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dbf66a",
   "metadata": {},
   "source": [
    "And then our **Fraud *ParallelPipeline*** (noting that one of the steps in this pipeline is the **Fraud Rule Generation *LinearPipeline*** created above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "603756f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_gen_lp = ParallelPipeline(\n",
    "    steps = [\n",
    "        ('fraud_gen_lp', fraud_gen_lp),\n",
    "        ('fraud_opt', fraud_opt),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ef944",
   "metadata": {},
   "source": [
    "And then finally, our **Fraud *LinearPipeline***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fb506ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_lp = LinearPipeline(\n",
    "    steps = [\n",
    "        ('fraud_gen_lp', fraud_gen_lp),\n",
    "        ('fraud_sf', fraud_sf),\n",
    "        ('fraud_cf', fraud_cf),\n",
    "#         ('fraud_gf', fraud_gf)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eccc72",
   "metadata": {},
   "source": [
    "Now we can do the same for the **Good *LinearPipeline***:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032730fd",
   "metadata": {},
   "source": [
    "### Good *LinearPipeline*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d9ff2b",
   "metadata": {},
   "source": [
    "Let's first instantiate the classes that we'll use in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfa75038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule generation\n",
    "good_gen = RuleGeneratorDT(\n",
    "    metric=f1.fit,\n",
    "    n_total_conditions=4,\n",
    "    tree_ensemble=RandomForestClassifier(\n",
    "        n_estimators=10,\n",
    "        random_state=0\n",
    "    ),\n",
    "    target_feat_corr_types='Infer',\n",
    "    rule_name_prefix='Approve' # Set this so generated reject rules distinguishable from approve rules\n",
    ")\n",
    "\n",
    "# Rule optimisation (for generated rules)\n",
    "good_gen_opt = BayesianOptimiser(\n",
    "    rule_lambdas=ClassAccessor(\n",
    "        class_tag='good_gen',\n",
    "        class_attribute='rule_lambdas'\n",
    "    ),\n",
    "    lambda_kwargs=ClassAccessor(\n",
    "        class_tag='good_gen',\n",
    "        class_attribute='lambda_kwargs'\n",
    "    ),\n",
    "    metric=f1.fit,\n",
    "    n_iter=10\n",
    ")\n",
    "\n",
    "# Rule optimisation (for existing rules)\n",
    "good_opt = BayesianOptimiser(\n",
    "    rule_lambdas=good_rule_lambdas,\n",
    "    lambda_kwargs=good_rules.lambda_kwargs,\n",
    "    metric=f1.fit,\n",
    "    n_iter=10\n",
    ")\n",
    "\n",
    "# Rule filter (performance-based)\n",
    "good_sf = SimpleFilter(\n",
    "    threshold=0.1, \n",
    "    operator='>=', \n",
    "    metric=f1.fit\n",
    ")\n",
    "\n",
    "# Rule filter (correlation-based)\n",
    "js = JaccardSimilarity()\n",
    "good_cf = CorrelatedFilter(\n",
    "    correlation_reduction_class=AgglomerativeClusteringReducer(\n",
    "        threshold=0.9, \n",
    "        strategy='top_down', \n",
    "        similarity_function=js.fit, \n",
    "        metric=f1.fit\n",
    "    ),\n",
    "    rules=ClassAccessor(\n",
    "        class_tag='good_gen',\n",
    "        class_attribute='rules'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Rule filter (greedy-based)\n",
    "good_gf = GreedyFilter(\n",
    "    metric=f1.fit,\n",
    "    sorting_metric=p.fit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb5ba1",
   "metadata": {},
   "source": [
    "Now we can create our **Good Rule Generation *LinearPipeline***. Note that we pass the tag for the optimisation of the generated rules to the `use_init_data` parameter, so that the feature set is passed to the `BayesianOptimiser` class, rather than the output from the `RuleGeneratorDT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "250aa191",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_gen_lp = LinearPipeline(\n",
    "    steps = [\n",
    "        ('good_gen', good_gen),\n",
    "        ('good_gen_opt', good_gen_opt),\n",
    "    ],\n",
    "    use_init_data=['good_gen_opt']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6533905",
   "metadata": {},
   "source": [
    "And then our **Good *ParallelPipeline*** (noting that one of the steps in this pipeline is the **Good Rule Generation *LinearPipeline*** created above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d09977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_gen_lp = ParallelPipeline(\n",
    "    steps = [\n",
    "        ('good_gen_lp', good_gen_lp),\n",
    "        ('good_opt', good_opt),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ddc8a",
   "metadata": {},
   "source": [
    "And then finally, our **Good *LinearPipeline***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eefd9e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_lp = LinearPipeline(\n",
    "    steps = [\n",
    "        ('good_gen_lp', good_gen_lp),\n",
    "        ('good_sf', good_sf),\n",
    "        ('good_cf', good_cf),\n",
    "#         ('good_gf', fraud_gf)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da31d9e",
   "metadata": {},
   "source": [
    "Now we can move on to constructing the **Overall Pipelines:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee07f23",
   "metadata": {},
   "source": [
    "### Overall Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ade135",
   "metadata": {},
   "source": [
    "First, we'll construct our **Overall *ParallelPipeline*** using the **Fraud *LinearPipeline*** and **Good *LinearPipeline***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbbf596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_pp = ParallelPipeline(\n",
    "    steps = [\n",
    "        ('fraud_lp', fraud_lp),\n",
    "        ('good_lp', good_lp)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d6118b",
   "metadata": {},
   "source": [
    "Now we can instantiate the decision engine optimiser. Since we have a constraint on the decision engine (if any approve rules fire, approve the transaction; else if any reject rules fire, reject the transaction; else approve the transaction), we pass the rules remaining after the filtering stages to the relevant elements in the `config` parameter of the `RBSPipeline` class, using the `ClassAccessor` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1ad4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision engine optimisation metric\n",
    "rev = Revenue(\n",
    "    y_type='Fraud',\n",
    "    chargeback_multiplier=3\n",
    ")\n",
    "\n",
    "# Decision engine (to be optimised)\n",
    "rbs_pipeline = RBSPipeline(\n",
    "    config=[\n",
    "        [\n",
    "            0, ClassAccessor( # If any approve rules fire, approve\n",
    "                class_tag='good_gf', \n",
    "                class_attribute='rules_to_keep'\n",
    "            ),\n",
    "        ],\n",
    "        [\n",
    "            1, ClassAccessor( # Else if any reject rules fire, reject\n",
    "                class_tag='fraud_gf', \n",
    "                class_attribute='rules_to_keep'\n",
    "            )\n",
    "        ],        \n",
    "    ],\n",
    "    final_decision=0 # Else approve\n",
    ")\n",
    "\n",
    "# Decision engine optimiser\n",
    "rbs_optimiser = RBSOptimiser(\n",
    "    pipeline=rbs_pipeline,\n",
    "    metric=rev.fit,     \n",
    "    rules=ClassAccessor(\n",
    "        class_tag='overall_pp',\n",
    "        class_attribute='rules'\n",
    "    ),\n",
    "    n_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71d3a8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rule_dicts': {},\n",
       " 'rule_strings': {},\n",
       " 'rule_lambdas': {},\n",
       " 'lambda_kwargs': {},\n",
       " 'lambda_args': {},\n",
       " 'rule_features': {},\n",
       " 'orig_rule_lambdas': {'ExistingReject1': <function iguanas.rules._convert_rule_dicts_to_rule_strings._ConvertRuleDictsToRuleStrings._convert_to_lambda.<locals>._make_lambda.<locals>.<lambda>(**kwargs)>,\n",
       "  'ExistingReject2': <function iguanas.rules._convert_rule_dicts_to_rule_strings._ConvertRuleDictsToRuleStrings._convert_to_lambda.<locals>._make_lambda.<locals>.<lambda>(**kwargs)>},\n",
       " 'orig_lambda_kwargs': {'ExistingReject1': {'V1': 0, 'V3': 1},\n",
       "  'ExistingReject2': {'V2': 3}},\n",
       " 'metric': <bound method FScore.fit of FScore with beta=1>,\n",
       " 'n_iter': 10,\n",
       " 'algorithm': <function hyperopt.tpe.suggest(new_ids, domain, trials, seed, prior_weight=1.0, n_startup_jobs=20, n_EI_candidates=24, gamma=0.25, verbose=True)>,\n",
       " 'verbose': 0,\n",
       " 'num_cores': 1,\n",
       " 'kwargs': {},\n",
       " 'rule_names': []}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_opt.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf408bd",
   "metadata": {},
   "source": [
    "Finally, we can instantiate our **Overall *LinearPipeline***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "979fa2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_lp = LinearPipeline(\n",
    "    steps=[\n",
    "        ('overall_pp', overall_pp),\n",
    "        ('rbs_optimiser', rbs_optimiser)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b040d",
   "metadata": {},
   "source": [
    "## Define the search space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed60794",
   "metadata": {},
   "source": [
    "Now we need to define the search space for each of the relevant parameters of our pipeline. **Note:** this example does not search across all hyperparameters - you should define your own search spaces based on your use case.\n",
    "\n",
    "To do this, we create a dictionary, where each key corresponds to the tag used for the relevant pipeline step. Each value should be a dictionary of the parameters (keys) and their search spaces (values). Search spaces should be defined using the classes in the `iguanas.space` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81da6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additional FScores\n",
    "f0dot5 = FScore(beta=0.5)\n",
    "f0dot25 = FScore(beta=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "218a3ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_spaces = {\n",
    "    'fraud_gen': {\n",
    "        'n_total_conditions': UniformInteger(2, 5),\n",
    "    },\n",
    "    'fraud_gen_opt': {\n",
    "        'metric': Choice([f0dot25.fit, f0dot5.fit, f1.fit]),\n",
    "    },\n",
    "    'fraud_sf': {\n",
    "        'threshold': UniformFloat(0, 1),\n",
    "    },\n",
    "    'fraud_cf': {\n",
    "        'correlation_reduction_class': Choice(\n",
    "            [\n",
    "                AgglomerativeClusteringReducer(threshold=0.9, \n",
    "        strategy='top_down', \n",
    "        similarity_function=js.fit, \n",
    "        metric=f1.fit\n",
    "    )\n",
    "    },    \n",
    "    'good_gen': {\n",
    "        'n_total_conditions': UniformInteger(2, 5),\n",
    "    },\n",
    "    'good_gen_opt': {\n",
    "        'metric': Choice([f0dot25.fit, f0dot5.fit, f1.fit]),\n",
    "    },\n",
    "    'good_sf': {\n",
    "        'threshold': UniformFloat(0, 1),\n",
    "    },\n",
    "    'good_cf': {\n",
    "        'threshold': UniformFloat(0, 1)\n",
    "    },    \n",
    "#     'good_gf': {\n",
    "#         'metric': Choice([f0dot25.fit, f0dot5.fit, f1.fit])\n",
    "#     },    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63e6476",
   "metadata": {},
   "source": [
    "## Optimise the pipeline hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80181bf4",
   "metadata": {},
   "source": [
    "Now that we have our pipeline and search spaces defined, we can instantiate the `BayesSearchCV` class. We'll split our data into 3 cross-validation datasets and try 20 different parameter sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2060a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BayesSearchCV(\n",
    "    pipeline=overall_lp, \n",
    "    search_spaces=search_spaces, \n",
    "    metric=f1.fit, \n",
    "    cv=3, \n",
    "    n_iter=20,\n",
    "    num_cores=3,\n",
    "    error_score=0,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee3ff2a",
   "metadata": {},
   "source": [
    "Finally, we can run the `fit` method to optimise the hyperparameters of the pipeline. \n",
    "\n",
    "**Note the following:** \n",
    "\n",
    "* The existing rules contain conditions that rely on unprocessed data (in this case, there are conditions that check for nulls). So for the rule optimisation steps, we must use the unprocessed training data `X_train_raw`; for the rule generation steps, we must use the processed training data `X_train`.\n",
    "* Since we're generating and optimising rules that flag both positive and negative cases (i.e. reject and approve rules in this example), we need to specify what the target is in each case. For the reject rules, we can just use `y_train`, however for the approve rules, we need to flip `y_train` (so that the target for the rule generator and rule optimisers is the negative cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af7f1e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Optimising pipeline parameters ---\n",
      "\r",
      "  0%|                                                                                                                                                                                                                                                                      | 0/20 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: Parameter `threshold` not found in keyword arguments for step `fraud_cf`\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                      | 0/20 [00:02<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parameter `threshold` not found in keyword arguments for step `fraud_cf`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hw/y20855_146x8gbvbpqmtpdp00000gq/T/ipykernel_26794/929544538.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bs.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     X={\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m'fraud_gen_lp'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m'fraud_opt'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m'good_gen_lp'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Iguanas/iguanas/rule_selection/bayes_search_cv.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--- Optimising pipeline parameters ---'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         self.best_params, self.cv_results = self._optimise_params(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mcv_datasets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0msearch_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_spaces_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Iguanas/iguanas/rule_selection/bayes_search_cv.py\u001b[0m in \u001b[0;36m_optimise_params\u001b[0;34m(self, cv_datasets, pipeline, search_spaces)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mobjective_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msearch_spaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         best_params = fmin(\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objective\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/iguanas_dev/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/iguanas_dev/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/iguanas_dev/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/iguanas_dev/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/iguanas_dev/lib/python3.8/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    905\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m             )\n\u001b[0;32m--> 907\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Iguanas/iguanas/rule_selection/bayes_search_cv.py\u001b[0m in \u001b[0;36m_objective\u001b[0;34m(self, objective_inputs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mparams_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;31m# Fit/predict/score on each fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Iguanas/iguanas/pipeline/_base_pipeline.py\u001b[0m in \u001b[0;36m_update_kwargs\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# If step inherits from _BasePipeline, call its _update_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_BasePipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep_tag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_tag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Iguanas/iguanas/pipeline/_base_pipeline.py\u001b[0m in \u001b[0;36m_update_kwargs\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# If step inherits from _BasePipeline, call its _update_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_BasePipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep_tag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_tag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Iguanas/iguanas/pipeline/_base_pipeline.py\u001b[0m in \u001b[0;36m_update_kwargs\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_tag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m     74\u001b[0m                             f'Parameter `{param}` not found in keyword arguments for step `{step_tag}`')\n\u001b[1;32m     75\u001b[0m                 \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_tag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parameter `threshold` not found in keyword arguments for step `fraud_cf`"
     ]
    }
   ],
   "source": [
    "bs.fit(\n",
    "    X={\n",
    "        'fraud_gen_lp': X_train,\n",
    "        'fraud_opt': X_train_raw,\n",
    "        'good_gen_lp': X_train,\n",
    "        'good_opt': X_train_raw    \n",
    "    }, \n",
    "    y={\n",
    "#         'fraud_gen_lp': y_train,\n",
    "#         'fraud_opt': y_train,\n",
    "        'fraud_lp': y_train,\n",
    "#         'good_gen_lp': 1-y_train,\n",
    "#         'good_opt': 1-y_train,\n",
    "        'good_lp': 1-y_train,\n",
    "        'rbs_optimiser': y_train\n",
    "    },\n",
    "    sample_weight={\n",
    "#         'fraud_gen_lp': y_train,\n",
    "#         'fraud_opt': y_train,\n",
    "        'fraud_lp': None,\n",
    "#         'good_gen_lp': 1-y_train,\n",
    "#         'good_opt': 1-y_train,\n",
    "        'good_lp': None,\n",
    "        'rbs_optimiser': y_train        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2486b3a4",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3ad83c",
   "metadata": {},
   "source": [
    "The `fit` method doesn't return anything. See the `Attributes` section in the class docstring for a description of each attribute generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4df48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e61e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cec55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.pipeline_.get_params()['rbs_optimiser']['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f29cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1.fit(bs.pipeline_.rules.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795ebe8",
   "metadata": {},
   "source": [
    "## Apply the optimised pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d86d2",
   "metadata": {},
   "source": [
    "We can apply our optimised pipeline to a new data set and make a prediction using the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f448c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = bs.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f9302",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23adbb1",
   "metadata": {},
   "source": [
    "The `predict` method returns the prediction generated by class in the final step of the pipeline - in this case, the `RBSOptimiser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe02d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491f459",
   "metadata": {},
   "source": [
    "We can now calculate the F1 score of our optimised pipeline using the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3fa768",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_opt = f1.fit(y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e4e70",
   "metadata": {},
   "source": [
    "Comparing this to our original, unoptimised pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a948cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.fit(X_train, y_train, None)\n",
    "y_pred_test_init = lp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d0a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_init = f1.fit(y_pred_test_init, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percentage improvement in F1 score: {round(100*(f1_opt-f1_init)/f1_init, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440a43f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a5a22224d030f6805b27da964f50b3905be89918ca593f843e32c3b2a80fa84"
  },
  "kernelspec": {
   "display_name": "iguanas_dev",
   "language": "python",
   "name": "iguanas_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
