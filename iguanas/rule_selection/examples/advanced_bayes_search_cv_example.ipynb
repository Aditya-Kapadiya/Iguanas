{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d32be5",
   "metadata": {},
   "source": [
    "# Advanced Bayes Search CV Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79fde41",
   "metadata": {},
   "source": [
    "This is a more advanced example of how the `BayesSearchCV` class can be applied - it's recommended that you first read through the simpler `bayes_search_cv_example`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be487a30",
   "metadata": {},
   "source": [
    "The `BayesSearchCV` class is used to search for the set of hyperparameters that produce the best decision engine performance for a given Iguanas Pipeline, whilst also reducing the likelihood of overfitting.\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "* Generate k-fold stratified cross validation datasets. \n",
    "* For each of the training and validation datasets:\n",
    "    * Fit the pipeline on the training set using a set of parameters chosen by the Bayesian Optimiser from a given set of ranges.\n",
    "    * Apply the pipeline to the validation set to return a prediction.\n",
    "    * Use the provided `scorer` to calculate the score of the prediction.\n",
    "* Return the parameter set which generated the highest mean overall score across the validation datasets.\n",
    "\n",
    "In this example, we'll consider the following more advanced workflow (compared to the standard `bayes_search_cv_example` notebook), which considers the generation of a Rules-Based System for a credit card fraud transaction use case:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accdca31",
   "metadata": {},
   "source": [
    "![title](images/complex_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63875eeb",
   "metadata": {},
   "source": [
    "Here, we have a fraud detection use case, and we're aiming to create two distinct rule sets - one for flagging fraudulent behaviour; one for flagging good behaviour. Each of these rule sets will be comprised of a generated rule set and an existing rule set. We'll optimise and filter these two rule sets separately, then combine and feed them into the decision engine optimiser. **Note:** we optimise the generated rules as they'll be created using the `RuleGeneratorDT` class, which generates rules from the branches of decision trees - these split based on gini or entropy - so we can further optimise them for a specific metric. \n",
    "\n",
    "**The decision engine will have the following constraint:** for a given transaction, if any approve rules fire it will be approved; else, if any reject rules fire it will be rejected; else, it will be approved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b9f4d",
   "metadata": {},
   "source": [
    "We'll use the `BayesSearchCV` class to optimise the hyperparameters of the steps in this workflow, **ensuring that we maximise the revenue for our decision engine.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aba9dc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438fcc3",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250846dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iguanas.rule_generation import RuleGeneratorDT\n",
    "from iguanas.rule_selection import SimpleFilter, CorrelatedFilter, GreedyFilter, BayesSearchCV\n",
    "from iguanas.metrics import FScore, Precision, Revenue, JaccardSimilarity\n",
    "from iguanas.rbs import RBSOptimiser, RBSPipeline\n",
    "from iguanas.correlation_reduction import AgglomerativeClusteringReducer\n",
    "from iguanas.pipeline import LinearPipeline, ParallelPipeline\n",
    "from iguanas.pipeline.class_accessor import ClassAccessor\n",
    "from iguanas.space import UniformFloat, UniformInteger, Choice\n",
    "from iguanas.rules import Rules\n",
    "from iguanas.rule_optimisation import BayesianOptimiser\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce42ec",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0492642",
   "metadata": {},
   "source": [
    "Let's read in the [credit card fraud dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud) from Kaggle.\n",
    "\n",
    "**Note:** this data has been altered to include some null values in the `V1` column. This is to simulate unprocessed data (the dataset on Kaggle has been processed using PCA, so there are no null values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3cba0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Class'\n",
    "time_col = 'Time'\n",
    "amt_col = 'Amount'\n",
    "# Ready in data\n",
    "df = pd.read_pickle('dummy_data/creditcard.pkl')\n",
    "# Sort data by time ascending\n",
    "df.sort_values(time_col, ascending=True)\n",
    "# Create X and y dataframes\n",
    "X = df.drop([target_col, time_col], axis=1)\n",
    "y = df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7bdbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca7c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "amts_train = X_train_raw[amt_col]\n",
    "amts_test = X_test_raw[amt_col]\n",
    "X_train_raw = X_train_raw.drop([amt_col], axis=1)\n",
    "X_test_raw = X_test_raw.drop([amt_col], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e2ea0",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd191a",
   "metadata": {},
   "source": [
    "Let's impute the null values with the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56faa994",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train_raw),\n",
    "    columns=X_train_raw.columns,\n",
    "    index=X_train_raw.index\n",
    ")\n",
    "X_test = pd.DataFrame(\n",
    "    imputer.transform(X_test_raw),\n",
    "    columns=X_test_raw.columns,\n",
    "    index=X_test_raw.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47717c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check nulls have been imputed\n",
    "X_train.isna().sum().sum(), X_test.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e472386",
   "metadata": {},
   "source": [
    "### Existing rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a19c9e",
   "metadata": {},
   "source": [
    "Let's also assume we have the following existing rules, stored in the standard Iguanas string format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcbebce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_rule_strings = {\n",
    "    \"ExistingReject1\": \"((X['V1']<0)|(X['V1'].isna()))&(X['V3']<1)\",\n",
    "    \"ExistingReject2\": \"(X['V2']>3)\",\n",
    "}\n",
    "good_rule_strings = {\n",
    "    \"ExistingApprove1\": \"(X['V1']>0)&(X['V3']>1)\",\n",
    "    \"ExistingApprove2\": \"(X['V2']<3)\",\n",
    "    \"ExistingApprove3\": \"(X['V4']<3)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8085939c",
   "metadata": {},
   "source": [
    "We can create a `Rules` class for each of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "604e4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_rules = Rules(rule_strings=fraud_rule_strings)\n",
    "good_rules = Rules(rule_strings=good_rule_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196fb16",
   "metadata": {},
   "source": [
    "Then convert them to the standard Iguanas lambda expression format (we'll need this for the optimisation step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b5823e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_rule_lambdas = fraud_rules.as_rule_lambdas(\n",
    "    as_numpy=False, \n",
    "    with_kwargs=True\n",
    ")\n",
    "good_rule_lambdas = good_rules.as_rule_lambdas(\n",
    "    as_numpy=False, \n",
    "    with_kwargs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e38dd",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc7364",
   "metadata": {},
   "source": [
    "## Set up pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67c9cc",
   "metadata": {},
   "source": [
    "Before we can apply the `BayesSearchCV` class, we need to set up our pipeline. To create the workflow shown at the beginning of the notebook, we must use a combination of `LinearPipeline` and `ParallelPipeline` classes as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e575f2",
   "metadata": {},
   "source": [
    "![title](images/complex_example_setup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a448c4",
   "metadata": {},
   "source": [
    "Let's begin building the **Fraud *LinearPipeline***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c92fc8",
   "metadata": {},
   "source": [
    "### Fraud *LinearPipeline*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907b388a",
   "metadata": {},
   "source": [
    "Let's first instantiate the classes that we'll use in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11021686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "f1 = FScore(beta=1)\n",
    "# Precision\n",
    "p = Precision()\n",
    "    \n",
    "# Rule generation\n",
    "fraud_gen = RuleGeneratorDT(\n",
    "    metric=f1.fit,\n",
    "    n_total_conditions=4,\n",
    "    tree_ensemble=RandomForestClassifier(\n",
    "        n_estimators=10,\n",
    "        random_state=0\n",
    "    ),\n",
    "    target_feat_corr_types='Infer',\n",
    "    rule_name_prefix='Reject' # Set this so generated reject rules distinguishable from approve rules\n",
    ")\n",
    "\n",
    "# Rule optimisation (for generated rules)\n",
    "fraud_gen_opt = BayesianOptimiser(\n",
    "    rule_lambdas=ClassAccessor(\n",
    "        class_tag='fraud_gen',\n",
    "        class_attribute='rule_lambdas'\n",
    "    ),\n",
    "    lambda_kwargs=ClassAccessor(\n",
    "        class_tag='fraud_gen',\n",
    "        class_attribute='lambda_kwargs'\n",
    "    ),\n",
    "    metric=f1.fit,\n",
    "    n_iter=10\n",
    ")\n",
    "\n",
    "# Rule optimisation (for existing rules)\n",
    "fraud_opt = BayesianOptimiser(\n",
    "    rule_lambdas=fraud_rule_lambdas,\n",
    "    lambda_kwargs=fraud_rules.lambda_kwargs,\n",
    "    metric=f1.fit,\n",
    "    n_iter=10\n",
    ")\n",
    "\n",
    "# Rule filter (performance-based)\n",
    "fraud_sf = SimpleFilter(\n",
    "    threshold=0.1, \n",
    "    operator='>=', \n",
    "    metric=f1.fit\n",
    ")\n",
    "\n",
    "# Rule filter (correlation-based)\n",
    "js = JaccardSimilarity()\n",
    "fraud_cf = CorrelatedFilter(\n",
    "    correlation_reduction_class=AgglomerativeClusteringReducer(\n",
    "        threshold=0.9, \n",
    "        strategy='top_down', \n",
    "        similarity_function=js.fit, \n",
    "        metric=f1.fit\n",
    "    ), \n",
    "    rules=ClassAccessor(\n",
    "        class_tag='fraud_gen',\n",
    "        class_attribute='rules'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Rule filter (greedy-based)\n",
    "fraud_gf = GreedyFilter(\n",
    "    metric=f1.fit,\n",
    "    sorting_metric=p.fit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5446d91",
   "metadata": {},
   "source": [
    "Now we can create our **Fraud Rule Generation *LinearPipeline***. Note that we pass the tag for the optimisation of the generated rules to the `use_init_data` parameter, so that the feature set is passed to the `BayesianOptimiser` class, rather than the output from the `RuleGeneratorDT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bb0c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_gen_lp = LinearPipeline(\n",
    "    steps = [\n",
    "        ('fraud_gen', fraud_gen),\n",
    "        ('fraud_gen_opt', fraud_gen_opt),\n",
    "    ],\n",
    "    use_init_data=['fraud_gen_opt']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1d3df",
   "metadata": {},
   "source": [
    "And then our **Fraud *ParallelPipeline*** (noting that one of the steps in this pipeline is the **Fraud Rule Generation *LinearPipeline*** created above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09ed59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_gen_lp = ParallelPipeline(\n",
    "    steps = [\n",
    "        ('fraud_gen_lp', fraud_gen_lp),\n",
    "        ('fraud_opt', fraud_opt),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f886739f",
   "metadata": {},
   "source": [
    "And then finally, our **Fraud *LinearPipeline***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7aa6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_lp = LinearPipeline(\n",
    "    steps = [\n",
    "        ('fraud_gen_lp', fraud_gen_lp),\n",
    "        ('fraud_sf', fraud_sf),\n",
    "        ('fraud_cf', fraud_cf),\n",
    "#         ('fraud_gf', fraud_gf)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7c2b6",
   "metadata": {},
   "source": [
    "Now we can do the same for the **Good *LinearPipeline***:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026dd64",
   "metadata": {},
   "source": [
    "### Good *LinearPipeline*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19d4a39",
   "metadata": {},
   "source": [
    "Let's first instantiate the classes that we'll use in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71a53f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule generation\n",
    "good_gen = RuleGeneratorDT(\n",
    "    metric=f1.fit,\n",
    "    n_total_conditions=4,\n",
    "    tree_ensemble=RandomForestClassifier(\n",
    "        n_estimators=10,\n",
    "        random_state=0\n",
    "    ),\n",
    "    target_feat_corr_types='Infer',\n",
    "    rule_name_prefix='Approve' # Set this so generated reject rules distinguishable from approve rules\n",
    ")\n",
    "\n",
    "# Rule optimisation (for generated rules)\n",
    "good_gen_opt = BayesianOptimiser(\n",
    "    rule_lambdas=ClassAccessor(\n",
    "        class_tag='good_gen',\n",
    "        class_attribute='rule_lambdas'\n",
    "    ),\n",
    "    lambda_kwargs=ClassAccessor(\n",
    "        class_tag='good_gen',\n",
    "        class_attribute='lambda_kwargs'\n",
    "    ),\n",
    "    metric=f1.fit,\n",
    "    n_iter=10\n",
    ")\n",
    "\n",
    "# Rule optimisation (for existing rules)\n",
    "good_opt = BayesianOptimiser(\n",
    "    rule_lambdas=good_rule_lambdas,\n",
    "    lambda_kwargs=good_rules.lambda_kwargs,\n",
    "    metric=f1.fit,\n",
    "    n_iter=10\n",
    ")\n",
    "\n",
    "# Rule filter (performance-based)\n",
    "good_sf = SimpleFilter(\n",
    "    threshold=0.1, \n",
    "    operator='>=', \n",
    "    metric=f1.fit\n",
    ")\n",
    "\n",
    "# Rule filter (correlation-based)\n",
    "js = JaccardSimilarity()\n",
    "good_cf = CorrelatedFilter(\n",
    "    correlation_reduction_class=AgglomerativeClusteringReducer(\n",
    "        threshold=0.9, \n",
    "        strategy='top_down', \n",
    "        similarity_function=js.fit, \n",
    "        metric=f1.fit\n",
    "    ),\n",
    "    rules=ClassAccessor(\n",
    "        class_tag='good_gen',\n",
    "        class_attribute='rules'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Rule filter (greedy-based)\n",
    "good_gf = GreedyFilter(\n",
    "    metric=f1.fit,\n",
    "    sorting_metric=p.fit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072832ee",
   "metadata": {},
   "source": [
    "Now we can create our **Good Rule Generation *LinearPipeline***. Note that we pass the tag for the optimisation of the generated rules to the `use_init_data` parameter, so that the feature set is passed to the `BayesianOptimiser` class, rather than the output from the `RuleGeneratorDT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4affe5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_gen_lp = LinearPipeline(\n",
    "    steps = [\n",
    "        ('good_gen', good_gen),\n",
    "        ('good_gen_opt', good_gen_opt),\n",
    "    ],\n",
    "    use_init_data=['good_gen_opt']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa50e50",
   "metadata": {},
   "source": [
    "And then our **Good *ParallelPipeline*** (noting that one of the steps in this pipeline is the **Good Rule Generation *LinearPipeline*** created above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83caa9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_gen_lp = ParallelPipeline(\n",
    "    steps = [\n",
    "        ('good_gen_lp', good_gen_lp),\n",
    "        ('good_opt', good_opt),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090903f7",
   "metadata": {},
   "source": [
    "And then finally, our **Good *LinearPipeline***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32715c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_lp = LinearPipeline(\n",
    "    steps = [\n",
    "        ('good_gen_lp', good_gen_lp),\n",
    "        ('good_sf', good_sf),\n",
    "        ('good_cf', good_cf),\n",
    "#         ('good_gf', fraud_gf)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeabac5",
   "metadata": {},
   "source": [
    "Now we can move on to constructing the **Overall Pipelines:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a41c9",
   "metadata": {},
   "source": [
    "### Overall Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e3842",
   "metadata": {},
   "source": [
    "First, we'll construct our **Overall *ParallelPipeline*** using the **Fraud *LinearPipeline*** and **Good *LinearPipeline***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96628f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_pp = ParallelPipeline(\n",
    "    steps = [\n",
    "        ('fraud_lp', fraud_lp),\n",
    "        ('good_lp', good_lp)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed04451",
   "metadata": {},
   "source": [
    "Now we can instantiate the decision engine optimiser. Since we have a constraint on the decision engine (if any approve rules fire, approve the transaction; else if any reject rules fire, reject the transaction; else approve the transaction), we pass the rules remaining after the filtering stages to the relevant elements in the `config` parameter of the `RBSPipeline` class, using the `ClassAccessor` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d67cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision engine optimisation metric\n",
    "rev = Revenue(\n",
    "    y_type='Fraud',\n",
    "    chargeback_multiplier=3\n",
    ")\n",
    "\n",
    "# Decision engine (to be optimised)\n",
    "rbs_pipeline = RBSPipeline(\n",
    "    config=[\n",
    "        [\n",
    "            0, ClassAccessor( # If any approve rules fire, approve\n",
    "                class_tag='good_cf', \n",
    "                class_attribute='rules_to_keep'\n",
    "            ),\n",
    "        ],\n",
    "        [\n",
    "            1, ClassAccessor( # Else if any reject rules fire, reject\n",
    "                class_tag='fraud_cf', \n",
    "                class_attribute='rules_to_keep'\n",
    "            )\n",
    "        ],        \n",
    "    ],\n",
    "    final_decision=0 # Else approve\n",
    ")\n",
    "\n",
    "# Decision engine optimiser\n",
    "rbs_optimiser = RBSOptimiser(\n",
    "    pipeline=rbs_pipeline,\n",
    "    metric=rev.fit,     \n",
    "    rules=ClassAccessor(\n",
    "        class_tag='overall_pp',\n",
    "        class_attribute='rules'\n",
    "    ),\n",
    "    n_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09edea08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rule_dicts': {},\n",
       " 'rule_strings': {},\n",
       " 'rule_lambdas': {},\n",
       " 'lambda_kwargs': {},\n",
       " 'lambda_args': {},\n",
       " 'rule_features': {},\n",
       " 'orig_rule_lambdas': {'ExistingReject1': <function iguanas.rules._convert_rule_dicts_to_rule_strings._ConvertRuleDictsToRuleStrings._convert_to_lambda.<locals>._make_lambda.<locals>.<lambda>(**kwargs)>,\n",
       "  'ExistingReject2': <function iguanas.rules._convert_rule_dicts_to_rule_strings._ConvertRuleDictsToRuleStrings._convert_to_lambda.<locals>._make_lambda.<locals>.<lambda>(**kwargs)>},\n",
       " 'orig_lambda_kwargs': {'ExistingReject1': {'V1': 0, 'V3': 1},\n",
       "  'ExistingReject2': {'V2': 3}},\n",
       " 'metric': <bound method FScore.fit of FScore with beta=1>,\n",
       " 'n_iter': 10,\n",
       " 'algorithm': <function hyperopt.tpe.suggest(new_ids, domain, trials, seed, prior_weight=1.0, n_startup_jobs=20, n_EI_candidates=24, gamma=0.25, verbose=True)>,\n",
       " 'verbose': 0,\n",
       " 'num_cores': 1,\n",
       " 'kwargs': {},\n",
       " 'rule_names': []}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_opt.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c6b96",
   "metadata": {},
   "source": [
    "Finally, we can instantiate our **Overall *LinearPipeline***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1810f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_lp = LinearPipeline(\n",
    "    steps=[\n",
    "        ('overall_pp', overall_pp),\n",
    "        ('rbs_optimiser', rbs_optimiser)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b040d",
   "metadata": {},
   "source": [
    "## Define the search space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed60794",
   "metadata": {},
   "source": [
    "Now we need to define the search space for each of the relevant parameters of our pipeline. **Note:** this example does not search across all hyperparameters - you should define your own search spaces based on your use case.\n",
    "\n",
    "To do this, we create a dictionary, where each key corresponds to the tag used for the relevant pipeline step. Each value should be a dictionary of the parameters (keys) and their search spaces (values). Search spaces should be defined using the classes in the `iguanas.space` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7087617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additional FScores\n",
    "f0dot5 = FScore(beta=0.5)\n",
    "f0dot25 = FScore(beta=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "218a3ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_spaces = {\n",
    "    'fraud_gen': {\n",
    "        'n_total_conditions': UniformInteger(2, 5),\n",
    "    },\n",
    "    'fraud_gen_opt': {\n",
    "        'metric': Choice([f0dot25.fit, f0dot5.fit, f1.fit]),\n",
    "    },\n",
    "    'fraud_sf': {\n",
    "        'threshold': UniformFloat(0, 1),\n",
    "    },\n",
    "    'fraud_cf': {\n",
    "        'correlation_reduction_class': Choice(\n",
    "            [\n",
    "                AgglomerativeClusteringReducer(\n",
    "                    threshold=0.9, \n",
    "                    strategy='top_down', \n",
    "                    similarity_function=js.fit, \n",
    "                    metric=f1.fit                    \n",
    "                ),\n",
    "                AgglomerativeClusteringReducer(\n",
    "                    threshold=0.95, \n",
    "                    strategy='top_down', \n",
    "                    similarity_function=js.fit, \n",
    "                    metric=f1.fit                    \n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    },    \n",
    "    'good_gen': {\n",
    "        'n_total_conditions': UniformInteger(2, 5),\n",
    "    },\n",
    "    'good_gen_opt': {\n",
    "        'metric': Choice([f0dot25.fit, f0dot5.fit, f1.fit]),\n",
    "    },\n",
    "    'good_sf': {\n",
    "        'threshold': UniformFloat(0, 1),\n",
    "    },\n",
    "    'good_cf': {\n",
    "        'correlation_reduction_class': Choice(\n",
    "            [\n",
    "                AgglomerativeClusteringReducer(\n",
    "                    threshold=0.9, \n",
    "                    strategy='top_down', \n",
    "                    similarity_function=js.fit, \n",
    "                    metric=f1.fit                    \n",
    "                ),\n",
    "                AgglomerativeClusteringReducer(\n",
    "                    threshold=0.95, \n",
    "                    strategy='top_down', \n",
    "                    similarity_function=js.fit, \n",
    "                    metric=f1.fit                    \n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    },    \n",
    "#     'good_gf': {\n",
    "#         'metric': Choice([f0dot25.fit, f0dot5.fit, f1.fit])\n",
    "#     },    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63e6476",
   "metadata": {},
   "source": [
    "## Optimise the pipeline hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80181bf4",
   "metadata": {},
   "source": [
    "Now that we have our pipeline and search spaces defined, we can instantiate the `BayesSearchCV` class. We'll split our data into 3 cross-validation datasets and try 20 different parameter sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2060a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BayesSearchCV(\n",
    "    pipeline=overall_lp, \n",
    "    search_spaces=search_spaces, \n",
    "    metric=rev.fit, # Use the same metric as the RBSOptimiser\n",
    "    cv=3, \n",
    "    n_iter=20,\n",
    "    num_cores=3,\n",
    "    error_score=0,\n",
    "    verbose=1,\n",
    "    sample_weight_in_val=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee3ff2a",
   "metadata": {},
   "source": [
    "Finally, we can run the `fit` method to optimise the hyperparameters of the pipeline. \n",
    "\n",
    "**Note the following:** \n",
    "\n",
    "* The existing rules contain conditions that rely on unprocessed data (in this case, there are conditions that check for nulls). So for the rule optimisation steps, we must use the unprocessed training data `X_train_raw`; for the rule generation steps, we must use the processed training data `X_train`.\n",
    "* Since we're generating and optimising rules that flag both positive and negative cases (i.e. reject and approve rules in this example), we need to specify what the target is in each case. For the reject rules, we can just use `y_train`, however for the approve rules, we need to flip `y_train` (so that the target for the rule generator and rule optimisers is the negative cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f1e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Optimising pipeline parameters ---\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [07:38<00:00, 22.93s/trial, best loss: -231.0]\n",
      "--- Refitting on entire dataset with best pipeline ---\n"
     ]
    }
   ],
   "source": [
    "bs.fit(\n",
    "    X={\n",
    "        'fraud_gen_lp': X_train,\n",
    "        'fraud_opt': X_train_raw,\n",
    "        'good_gen_lp': X_train,\n",
    "        'good_opt': X_train_raw    \n",
    "    }, \n",
    "    y={\n",
    "#         'fraud_gen_lp': y_train,\n",
    "#         'fraud_opt': y_train,\n",
    "        'fraud_lp': y_train,\n",
    "#         'good_gen_lp': 1-y_train,\n",
    "#         'good_opt': 1-y_train,\n",
    "        'good_lp': 1-y_train,\n",
    "        'rbs_optimiser': y_train\n",
    "    },\n",
    "    sample_weight={\n",
    "#         'fraud_gen_lp': y_train,\n",
    "#         'fraud_opt': y_train,\n",
    "        'fraud_lp': None,\n",
    "#         'good_gen_lp': 1-y_train,\n",
    "#         'good_opt': 1-y_train,\n",
    "        'good_lp': None,\n",
    "        'rbs_optimiser': y_train        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2486b3a4",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3ad83c",
   "metadata": {},
   "source": [
    "The `fit` method doesn't return anything. See the `Attributes` section in the class docstring for a description of each attribute generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4df48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e61e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.pipeline_.get_params()['rbs_optimiser']['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6fe3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1.fit(bs.pipeline_.rules.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795ebe8",
   "metadata": {},
   "source": [
    "## Apply the optimised pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d86d2",
   "metadata": {},
   "source": [
    "We can apply our optimised pipeline to a new data set and make a prediction using the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f448c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = bs.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f9302",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23adbb1",
   "metadata": {},
   "source": [
    "The `predict` method returns the prediction generated by class in the final step of the pipeline - in this case, the `RBSOptimiser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe02d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491f459",
   "metadata": {},
   "source": [
    "We can now calculate the F1 score of our optimised pipeline using the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3fa768",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_opt = f1.fit(y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e4e70",
   "metadata": {},
   "source": [
    "Comparing this to our original, unoptimised pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a948cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.fit(X_train, y_train, None)\n",
    "y_pred_test_init = lp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d0a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_init = f1.fit(y_pred_test_init, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percentage improvement in F1 score: {round(100*(f1_opt-f1_init)/f1_init, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440a43f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a5a22224d030f6805b27da964f50b3905be89918ca593f843e32c3b2a80fa84"
  },
  "kernelspec": {
   "display_name": "iguanas_dev",
   "language": "python",
   "name": "iguanas_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
