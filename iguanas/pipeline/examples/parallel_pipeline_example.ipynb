{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4314dd6f",
   "metadata": {},
   "source": [
    "# Parallel Pipeline Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a33f2",
   "metadata": {},
   "source": [
    "A Parallel Pipeline is a set of steps which run independently - their outputs are then combined and returned. Each step should be an instantiated class with both `fit` and `transform` methods.\n",
    "\n",
    "The below diagram shows the high level structure of a Parallel Pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb93c90a",
   "metadata": {},
   "source": [
    "![title](images/parallel_pipeline_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e68a3",
   "metadata": {},
   "source": [
    "It is used to carry out multiple processes seperately and combine their outputs. One such use case is when we want to both:\n",
    "\n",
    "1. Generate a new rule set, and:\n",
    "2. Optimise an existing rule set, then:\n",
    "3. Use the combined rule sets to optimise a decision engine\n",
    "\n",
    "An example of this workflow is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f124110",
   "metadata": {},
   "source": [
    "![title](images/parallel_pipeline_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626fcca6",
   "metadata": {},
   "source": [
    "The rule generation and rule optimisation steps would be added to a Parallel Pipeline, so they are run separately and their outputs are combined. This Parallel Pipeline would be added to a Linear Pipeline, along with the decision engine optimisation step, so that the output of the Parallel Pipeline is fed into the decision engine optimiser.\n",
    "\n",
    "**We'll see how this workflow can be generated in the following example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be721b29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc510f",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb7115b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iguanas.rule_generation import RuleGeneratorDT\n",
    "from iguanas.rule_optimisation import BayesianOptimiser\n",
    "from iguanas.rule_selection import SimpleFilter, CorrelatedFilter\n",
    "from iguanas.metrics import FScore, Precision, JaccardSimilarity\n",
    "from iguanas.rbs import RBSOptimiser, RBSPipeline\n",
    "from iguanas.correlation_reduction import AgglomerativeClusteringReducer\n",
    "from iguanas.pipeline import LinearPipeline, ParallelPipeline, ClassAccessor\n",
    "from iguanas.rules import Rules\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6031c6",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28354c2",
   "metadata": {},
   "source": [
    "Let's read in the famous Titanic data set and split it into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938ebd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../examples/dummy_data/titanic.csv', index_col='PassengerId')\n",
    "target_col = 'Survived'\n",
    "cols_to_drop = ['Name', 'Ticket', 'Cabin']\n",
    "X = df.drop([target_col] + cols_to_drop, axis=1)\n",
    "y = df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3ee24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0499265",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae50c1c2",
   "metadata": {},
   "source": [
    "Let's apply the following simple steps to process the data:\n",
    "* One hot encode categorical variables (accounting for nulls)\n",
    "* Impute numeric features with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac7d3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlaidler/venvs/iguanas_dev/lib/python3.8/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "# OHE\n",
    "encoder = OneHotEncoder(\n",
    "    use_cat_names=True\n",
    ")\n",
    "X_train = encoder.fit_transform(X_train_raw)\n",
    "X_test = encoder.transform(X_test_raw)\n",
    "\n",
    "# Impute\n",
    "X_train.fillna(-1, inplace=True)\n",
    "X_test.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a6921",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d043f0",
   "metadata": {},
   "source": [
    "## Set up pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c93934",
   "metadata": {},
   "source": [
    "To create the worflow shown at the beginning of the notebook, let's first assume we have the following existing rule set (stored in the standard Iguanas string format):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18ee4151",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_rules = Rules(\n",
    "    rule_strings = {\n",
    "        'AgeRule': \"(X['Age']>0)|(X['Age'].isna())\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e00c6",
   "metadata": {},
   "source": [
    "**Note:** these rules use the unprocessed data, as they contain conditions that look for null values. We'll need to use the unprocessed data when optimising these rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e8f2e",
   "metadata": {},
   "source": [
    "To use in a rule optimiser, we need to convert the rules to the standard Iguanas lambda expression format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829054c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_rule_lambdas = existing_rules.as_rule_lambdas(\n",
    "    as_numpy=False, \n",
    "    with_kwargs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7597389",
   "metadata": {},
   "source": [
    "Now let's set up our Parallel Pipeline - this will consist of a rule generation step and a rule optimisation step. We'll optimise both our rules and our decision engine based on the **F1 score**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "443296ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = FScore(beta=1)\n",
    "\n",
    "# Rule generation\n",
    "generator = RuleGeneratorDT(\n",
    "    metric=f1.fit,\n",
    "    n_total_conditions=4,\n",
    "    tree_ensemble=RandomForestClassifier(\n",
    "        n_estimators=10,\n",
    "        random_state=0\n",
    "    )\n",
    ")\n",
    "# Rule optimisation\n",
    "optimiser = BayesianOptimiser(\n",
    "    rule_lambdas=existing_rule_lambdas,\n",
    "    lambda_kwargs=existing_rules.lambda_kwargs,\n",
    "    metric=f1.fit,\n",
    "    n_iter=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7f0bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = ParallelPipeline(\n",
    "    steps=[\n",
    "        ('generator', generator),\n",
    "        ('optimiser', optimiser)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf2c31",
   "metadata": {},
   "source": [
    "Now that we have our Parallel Pipeline defined, we can define our Linear Pipeline, the first step of which will be our Parallel Pipeline. This will run the rule generation and optimisation steps separately, combine their outputs, and feed it into the decision engine optimiser (our second step in the Linear Pipeline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f39fe65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision engine optimiser\n",
    "rbs_pipeline = RBSPipeline(\n",
    "    config=[], # Use an empty list here - the RBSOptimiser will create the config\n",
    "    final_decision=0\n",
    ")\n",
    "rbs_optimiser = RBSOptimiser(\n",
    "    pipeline=rbs_pipeline,\n",
    "    metric=f1.fit, \n",
    "    pos_pred_rules=ClassAccessor(\n",
    "        class_tag='pp', \n",
    "        class_attribute='rule_names'\n",
    "    ),\n",
    "    n_iter=10,\n",
    "    rules=\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "086cb1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = LinearPipeline(\n",
    "    steps=[\n",
    "        ('pp', pp),\n",
    "        ('rbs_optimiser', rbs_optimiser)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb82d6c4",
   "metadata": {},
   "source": [
    "**Note:** The argument passed to the `pos_pred_rules` parameter in the `RBSOptimiser` class is a `ClassAccessor` object. This takes the names of the rules that are present in the concatenated output produced by the `ParallelPipeline` and passes it to the `pos_pred_rules` parameter of the `RBSOptimiser` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22183400",
   "metadata": {},
   "source": [
    "## Using the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f553b7",
   "metadata": {},
   "source": [
    "### `fit` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c398b",
   "metadata": {},
   "source": [
    "By running the `fit` method, we sequentially run the `fit_transform` methods of each step in the pipeline, except for the last step, where the `fit` method is run. \n",
    "\n",
    "**Note:** we need to pass the unprocessed data to the rule optimiser step - we can do this by feeding a dictionary to the parameter `X`, where the key of the dictionary corresponds to the step where the given dataset (value) should be passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9041d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.fit(\n",
    "    X={\n",
    "        'generator': X_train,\n",
    "        'optimiser': X_train_raw,\n",
    "    }, \n",
    "    y=y_train, \n",
    "    sample_weight=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c43da",
   "metadata": {},
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173994d",
   "metadata": {},
   "source": [
    "The `fit` method doesn't return anything. However, you can access the attributes of the fitted classes using the `get_params` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a472f",
   "metadata": {},
   "source": [
    "To see the rules that remain after the decision engine optimisation, we first need to extract the `rules_to_keep` attribute from the `rbso_optimiser` stage of the Linear Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d42b8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_to_keep = lp.get_params()['rbs_optimiser']['rules_to_keep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.get_params()['generator']['rule_strings']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab23e54",
   "metadata": {},
   "source": [
    "### `fit_predict` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2d716",
   "metadata": {},
   "source": [
    "By running the `fit_predict` method, we sequentially run the `fit_transform` methods of each step in the pipeline, except for the last step, where the `fit_predict` method is run.\n",
    "\n",
    "**Note:** we need to pass the unprocessed data to the rule optimiser step - we can do this by feeding a dictionary to the parameter `X`, where the key of the dictionary corresponds to the step where the given dataset (value) should be passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "363b1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = lp.fit_predict(\n",
    "    X={\n",
    "        'generator': X_train,\n",
    "        'optimiser': X_train_raw,\n",
    "    }, \n",
    "    y=y_train, \n",
    "    sample_weight=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0adb3a",
   "metadata": {},
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca32c00",
   "metadata": {},
   "source": [
    "The `fit_predict` method returns the prediction generated by class in the final step of the pipeline - in this case, the `RBSOptimiser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7967465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "591    1\n",
       "592    1\n",
       "593    1\n",
       "594    1\n",
       "595    1\n",
       "Name: Stage=0, Decision=1, Length: 596, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de8782",
   "metadata": {},
   "source": [
    "### `predict` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb64136f",
   "metadata": {},
   "source": [
    "By running the `predict` method, we sequentially run the `transform` methods of each step in the pipeline, except for the last step, where the `predict` method is run. Note that before using this method, you should first run either the `fit` or `fit_predict` methods:\n",
    "\n",
    "**Note:** we need to pass the unprocessed data to the rule optimiser step - we can do this by feeding a dictionary to the parameter `X`, where the key of the dictionary corresponds to the step where the given dataset (value) should be passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75773514",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = lp.predict(\n",
    "    X={\n",
    "        'generator': X_test,\n",
    "        'optimiser': X_test_raw,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce9eec",
   "metadata": {},
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe3abd",
   "metadata": {},
   "source": [
    "The `predict` method returns the prediction generated by class in the final step of the pipeline - in this case, the `RBSOptimiser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "653aee49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "290    1\n",
       "291    1\n",
       "292    1\n",
       "293    1\n",
       "294    1\n",
       "Name: Stage=0, Decision=1, Length: 295, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd0fe7",
   "metadata": {},
   "source": [
    "This approach is very powerful when optimising hyperparameters for the overall performance of a Rules-Based System - see the `BayesSearchCV` class in the `rule_selection` module for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b05e17",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a5a22224d030f6805b27da964f50b3905be89918ca593f843e32c3b2a80fa84"
  },
  "kernelspec": {
   "display_name": "iguanas_dev",
   "language": "python",
   "name": "iguanas_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
